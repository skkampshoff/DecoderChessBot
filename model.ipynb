{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8093ab15",
   "metadata": {},
   "source": [
    "# Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed244937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db7c8a9",
   "metadata": {},
   "source": [
    "# Decoder Class Definition\n",
    "Below is the defintion of our decder class that will be used to generate chess moves. Our decoder uses PyTorch's TransformerDecoder that incorporates multi-head self attention and feedforward neural nets, while adding and normalizing after each layer. Postional embeddings are calculated before being fed into the decoder layer. A fully connected layer is used for the output to calculate a softmax for the most probable chess moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19c4db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=8, num_layers=4, max_len=200):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=1024)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, seq_len] â†’ transformer expects [seq_len, batch]\n",
    "        x = x.transpose(0, 1)\n",
    "        seq_len, batch_size = x.size()\n",
    "\n",
    "        # Calculate positional embeddings\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(1)\n",
    "        x = self.embed(x) + self.pos_embed(positions)\n",
    "\n",
    "        # Decoder masking: prevent attention to future tokens\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()\n",
    "\n",
    "        x = self.decoder(x, x, tgt_mask=mask)\n",
    "        logits = self.fc_out(x)  # [seq_len, batch, vocab_size]\n",
    "        return logits.transpose(0, 1)  # [batch, seq_len, vocab_size]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087516e5",
   "metadata": {},
   "source": [
    "Pulling in the .pt file that contains the tensor of all encoded chess games that will be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3a56877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([500000, 200])\n",
      "tensor([    1, 10536, 10644, 10609, 10542, 10683,   303,  1791,  1347,  1338])\n"
     ]
    }
   ],
   "source": [
    "encoded_tensor = torch.load(\"encoded_games_500k.pt\")\n",
    "\n",
    "print(type(encoded_tensor))\n",
    "print(encoded_tensor.shape)\n",
    "print(encoded_tensor[0][:10])  # first few move IDs of first game\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888bc1e1",
   "metadata": {},
   "source": [
    "Defining the dataset we will use during training to store and pull game moves. Also intializing dataset and data loader with tensor containing the encoded chess games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b37bbc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, encoded_tensor):\n",
    "        self.data = encoded_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx][:-1]  # all but last\n",
    "        y = self.data[idx][1:]   # all but first\n",
    "        return x, y\n",
    "\n",
    "dataset = ChessDataset(encoded_tensor)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139950da",
   "metadata": {},
   "source": [
    "The next few cells are used to test the funtionality of the loader and model when working together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa2ed5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"move_to_id.json\", \"r\") as f:\n",
    "    move_to_id = json.load(f)\n",
    "\n",
    "vocab_size = len(move_to_id)\n",
    "model = ChessDecoder(vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2736011f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape: torch.Size([64, 199])\n",
      "Target batch shape: torch.Size([64, 199])\n",
      "Example input sequence: tensor([    1, 10533, 10539, 10638, 10644,  1475,  1647,  1632,  1490,  1941])\n",
      "Example target sequence: tensor([10533, 10539, 10638, 10644,  1475,  1647,  1632,  1490,  1941, 10755])\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(loader))\n",
    "\n",
    "print(\"Input batch shape:\", x.shape)\n",
    "print(\"Target batch shape:\", y.shape)\n",
    "print(\"Example input sequence:\", x[0][:10])\n",
    "print(\"Example target sequence:\", y[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e98038f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output shape: torch.Size([64, 199, 11017])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(x)\n",
    "print(\"Model output shape:\", logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "763f3038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.009690499864518642\n"
     ]
    }
   ],
   "source": [
    "PAD_ID = move_to_id['<PAD>']\n",
    "import torch.nn.functional as F\n",
    "\n",
    "loss = F.cross_entropy(\n",
    "    logits.reshape(-1, vocab_size),\n",
    "    y.reshape(-1),\n",
    "    ignore_index=PAD_ID\n",
    ")\n",
    "print(\"Test loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058e710e",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization\n",
    "We will use optuna's functionality on our datadset to tune hyperparameters before training on our larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bb32421",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorDataset, random_split, DataLoader\n\u001b[0;32m----> 3\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.9\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mdataset\u001b[49m))\n\u001b[1;32m      4\u001b[0m val_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset) \u001b[38;5;241m-\u001b[39m train_size\n\u001b[1;32m      5\u001b[0m train_dataset, val_dataset \u001b[38;5;241m=\u001b[39m random_split(dataset, [train_size, val_size])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'device {device} found')\n",
    "vocab_size = len(move_to_id)\n",
    "\n",
    "def objective(trial):\n",
    "    # search space\n",
    "    d_model = trial.suggest_categorical('d_model', [128, 256, 512])\n",
    "    nhead = trial.suggest_categorical('nhead', [4,8])\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 6)\n",
    "    lr = trial.suggest_float('lr', 1e-4, 5e-3)\n",
    "\n",
    "    # create model\n",
    "    model = ChessDecoder(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=d_model,\n",
    "        nhead=nhead,\n",
    "        num_layers=num_layers,\n",
    "        max_len=encoded_tensor.size(1)\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
    "\n",
    "    # train a few epochs\n",
    "    EPOCHS = 7\n",
    "    for epoch in range(EPOCHS):\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits.reshape(-1, vocab_size), y.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y, in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits.reshape(-1,vocab_size), y.reshape(-1))\n",
    "            total_val_loss += loss.item()\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    return avg_val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8d2c13",
   "metadata": {},
   "source": [
    "run optimization with optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e36140e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-20 14:06:50,773] A new study created in memory with name: no-name-e1b65a99-cda1-4333-adc6-754f06380a97\n",
      "[I 2025-10-20 14:12:42,695] Trial 0 finished with value: 0.0073929192708769385 and parameters: {'d_model': 128, 'nhead': 4, 'num_layers': 6, 'lr': 0.000980025811877717}. Best is trial 0 with value: 0.0073929192708769385.\n",
      "[I 2025-10-20 14:19:51,837] Trial 1 finished with value: 0.005282709680870586 and parameters: {'d_model': 512, 'nhead': 8, 'num_layers': 3, 'lr': 0.0007545065766000772}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 14:23:37,924] Trial 2 finished with value: 0.01237118748875405 and parameters: {'d_model': 128, 'nhead': 4, 'num_layers': 3, 'lr': 0.00261027736598459}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 14:28:07,065] Trial 3 finished with value: 0.005774908191084158 and parameters: {'d_model': 256, 'nhead': 8, 'num_layers': 3, 'lr': 0.0006363415651286342}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 14:31:37,107] Trial 4 finished with value: 0.1770311387685629 and parameters: {'d_model': 256, 'nhead': 8, 'num_layers': 2, 'lr': 0.0030208238577302533}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 14:42:01,424] Trial 5 finished with value: 6.243446313417875 and parameters: {'d_model': 512, 'nhead': 8, 'num_layers': 5, 'lr': 0.004786979976157613}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 14:50:48,485] Trial 6 finished with value: 0.006250375005877756 and parameters: {'d_model': 512, 'nhead': 8, 'num_layers': 4, 'lr': 0.00022567886959536012}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 14:56:16,856] Trial 7 finished with value: 6.3789984324039555 and parameters: {'d_model': 512, 'nhead': 8, 'num_layers': 2, 'lr': 0.0018549653907606448}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 15:04:55,583] Trial 8 finished with value: 6.301171528987395 and parameters: {'d_model': 512, 'nhead': 4, 'num_layers': 4, 'lr': 0.0039224699464368105}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 15:12:02,865] Trial 9 finished with value: 9.893798461327187 and parameters: {'d_model': 512, 'nhead': 8, 'num_layers': 3, 'lr': 0.0015884141376726452}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 15:17:31,661] Trial 10 finished with value: 9.66213366924188 and parameters: {'d_model': 128, 'nhead': 4, 'num_layers': 5, 'lr': 0.0016912410242492324}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 15:22:00,819] Trial 11 finished with value: 0.030505022404190056 and parameters: {'d_model': 256, 'nhead': 8, 'num_layers': 3, 'lr': 0.00014670889453875747}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 15:26:29,947] Trial 12 finished with value: 0.006386725969675284 and parameters: {'d_model': 256, 'nhead': 8, 'num_layers': 3, 'lr': 0.000842914083229808}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 15:29:59,298] Trial 13 finished with value: 0.006516576564769285 and parameters: {'d_model': 256, 'nhead': 8, 'num_layers': 2, 'lr': 0.0009096195204661793}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 15:35:25,810] Trial 14 finished with value: 11.660316455058563 and parameters: {'d_model': 256, 'nhead': 8, 'num_layers': 4, 'lr': 0.002208262218402548}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 15:42:34,088] Trial 15 finished with value: 11.834812200986422 and parameters: {'d_model': 512, 'nhead': 8, 'num_layers': 3, 'lr': 0.0010902975180027083}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 15:48:59,119] Trial 16 finished with value: 9.034021169711382 and parameters: {'d_model': 256, 'nhead': 8, 'num_layers': 5, 'lr': 0.003327028236190427}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 15:54:27,779] Trial 17 finished with value: 0.005541814316716344 and parameters: {'d_model': 512, 'nhead': 8, 'num_layers': 2, 'lr': 0.0003477923203994368}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 15:59:53,356] Trial 18 finished with value: 0.08012786748795173 and parameters: {'d_model': 512, 'nhead': 4, 'num_layers': 2, 'lr': 0.0013816511214314233}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 16:05:22,374] Trial 19 finished with value: 0.0055433202887331555 and parameters: {'d_model': 512, 'nhead': 8, 'num_layers': 2, 'lr': 0.00035070405967919146}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 16:10:51,080] Trial 20 finished with value: 6.736958075792361 and parameters: {'d_model': 512, 'nhead': 8, 'num_layers': 2, 'lr': 0.0021993964671356793}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 16:16:20,007] Trial 21 finished with value: 0.00559213283234455 and parameters: {'d_model': 512, 'nhead': 8, 'num_layers': 2, 'lr': 0.0003603890712253631}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 16:21:49,478] Trial 22 finished with value: 0.005973413218714249 and parameters: {'d_model': 512, 'nhead': 8, 'num_layers': 2, 'lr': 0.0005955190840257043}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 16:28:56,815] Trial 23 finished with value: 11.089595831357515 and parameters: {'d_model': 512, 'nhead': 8, 'num_layers': 3, 'lr': 0.0012975503088299622}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 16:34:26,051] Trial 24 finished with value: 0.013669978339189233 and parameters: {'d_model': 512, 'nhead': 8, 'num_layers': 2, 'lr': 0.00011382331065457927}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 16:41:34,661] Trial 25 finished with value: 0.005497399399339487 and parameters: {'d_model': 512, 'nhead': 8, 'num_layers': 3, 'lr': 0.0005700212284165392}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 16:45:53,576] Trial 26 finished with value: 0.010975553784728384 and parameters: {'d_model': 128, 'nhead': 4, 'num_layers': 4, 'lr': 0.0006315639864247946}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 16:53:02,492] Trial 27 finished with value: 10.323417235643436 and parameters: {'d_model': 512, 'nhead': 8, 'num_layers': 3, 'lr': 0.0019396238519622988}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 17:01:48,488] Trial 28 finished with value: 9.181424984565147 and parameters: {'d_model': 512, 'nhead': 8, 'num_layers': 4, 'lr': 0.0012804513717959488}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 17:08:18,441] Trial 29 finished with value: 0.008351677174244124 and parameters: {'d_model': 128, 'nhead': 4, 'num_layers': 6, 'lr': 0.0008234477901491895}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 17:15:26,666] Trial 30 finished with value: 11.76278417538374 and parameters: {'d_model': 512, 'nhead': 8, 'num_layers': 3, 'lr': 0.0011632748083813638}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 17:20:55,159] Trial 31 finished with value: 0.0055523332786180666 and parameters: {'d_model': 512, 'nhead': 8, 'num_layers': 2, 'lr': 0.00048247763929534977}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 17:26:25,803] Trial 32 finished with value: 0.006231399749641498 and parameters: {'d_model': 512, 'nhead': 8, 'num_layers': 2, 'lr': 0.0003990837295662488}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 17:33:34,734] Trial 33 finished with value: 0.010777445222349623 and parameters: {'d_model': 512, 'nhead': 8, 'num_layers': 3, 'lr': 0.0007538239025423162}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 17:36:22,320] Trial 34 finished with value: 0.009290768841543841 and parameters: {'d_model': 128, 'nhead': 8, 'num_layers': 2, 'lr': 0.0026631214295095252}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 17:43:30,873] Trial 35 finished with value: 0.005293759407066272 and parameters: {'d_model': 512, 'nhead': 8, 'num_layers': 3, 'lr': 0.0004308565999604102}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 17:50:33,777] Trial 36 finished with value: 8.555250314565805 and parameters: {'d_model': 512, 'nhead': 4, 'num_layers': 3, 'lr': 0.001006709427891155}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 17:59:20,637] Trial 37 finished with value: 6.25545947979658 and parameters: {'d_model': 512, 'nhead': 8, 'num_layers': 4, 'lr': 0.004569903153790165}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 18:06:28,315] Trial 38 finished with value: 8.413278604165102 and parameters: {'d_model': 512, 'nhead': 8, 'num_layers': 3, 'lr': 0.0014528511263466556}. Best is trial 1 with value: 0.005282709680870586.\n",
      "[I 2025-10-20 18:11:07,148] Trial 39 finished with value: 0.015743974657454647 and parameters: {'d_model': 128, 'nhead': 8, 'num_layers': 4, 'lr': 0.0005743397679553087}. Best is trial 1 with value: 0.005282709680870586.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Best trial:\n",
      "  Value (val loss): 0.005282709680870586\n",
      "  Params: {'d_model': 512, 'nhead': 8, 'num_layers': 3, 'lr': 0.0007545065766000772}\n"
     ]
    }
   ],
   "source": [
    "# set up optuna\n",
    "\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "# run a few rounds at random first\n",
    "sampler = TPESampler(n_startup_trials=5)\n",
    "pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=1)\n",
    "\n",
    "study = optuna.create_study(direction='minimize', sampler=sampler, pruner=pruner)\n",
    "study.optimize(objective, n_trials=40)\n",
    "\n",
    "# print results\n",
    "\n",
    "print('\\n\\nBest trial:')\n",
    "print('  Value (val loss):', study.best_trial.value)\n",
    "print('  Params:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13800d8",
   "metadata": {},
   "source": [
    "The following cell allows for easy storage and retrieval of the best hyperparameters as found above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fbde2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith open(\"best_hparams.json\", \"r\") as f:\\n    best_hparams = json.load(f)\\n\\nparams = best_hparams[\"best_params\"]\\n\\nbest_model = ChessDecoder(\\n    vocab_size=vocab_size,\\n    d_model=params[\"d_model\"],\\n    nhead=params[\"nhead\"],\\n    num_layers=params[\"num_layers\"],\\n    max_len=encoded_tensor.size(1)\\n).to(device)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save best hyperparameters to file\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "best_params = study.best_trial.params\n",
    "best_value = study.best_trial.value\n",
    "\n",
    "best_params_with_meta = {\n",
    "    \"best_params\": best_params,\n",
    "    \"best_val_loss\": best_value,\n",
    "    \"n_trials\": len(study.trials),\n",
    "}\n",
    "\n",
    "filename = f'best_hparams.json'\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(best_params_with_meta, f)\n",
    "\n",
    "\n",
    "# how to open later or in different file\n",
    "'''\n",
    "with open(\"best_hparams.json\", \"r\") as f:\n",
    "    best_hparams = json.load(f)\n",
    "\n",
    "params = best_hparams[\"best_params\"]\n",
    "\n",
    "best_model = ChessDecoder(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=params[\"d_model\"],\n",
    "    nhead=params[\"nhead\"],\n",
    "    num_layers=params[\"num_layers\"],\n",
    "    max_len=encoded_tensor.size(1)\n",
    ").to(device)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffd13dc",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "Training is done after the best combination of hyperparameters have been found. These hyperparameters are used to create the final instance of our model. This model is trained on an encoded dataset of 1 million chess games and uses cross entropy to calcuate loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2269c896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On device cuda\n",
      "Epoch 1 average loss = 0.0191\n",
      "Epoch 2 average loss = 0.0169\n",
      "Epoch 3 average loss = 0.0163\n",
      "Epoch 4 average loss = 0.0151\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     43\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 45\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     num_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     48\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m500k_model.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "vocab_size = len(move_to_id)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'On device {device}')\n",
    "\n",
    "with open(\"best_hparams.json\", \"r\") as f:\n",
    "    best_hparams = json.load(f)\n",
    "\n",
    "params = best_hparams[\"best_params\"]\n",
    "\n",
    "model = ChessDecoder(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=params[\"d_model\"],\n",
    "    nhead=params[\"nhead\"],\n",
    "    num_layers=params[\"num_layers\"],\n",
    "    max_len=encoded_tensor.size(1)\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('500k_model.pt', map_location=device))\n",
    "model.train()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "\n",
    "epoch_loss = 0\n",
    "num_batches = 0\n",
    "for epoch in range(50):\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(\n",
    "            logits.reshape(-1, vocab_size),\n",
    "            y.reshape(-1),\n",
    "            ignore_index=PAD_ID\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    torch.save(model.state_dict(), '500k_model.pt')\n",
    "    print(f\"Epoch {epoch+1} average loss = {epoch_loss / num_batches:.4f}\")\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
